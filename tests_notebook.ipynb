{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NUM_FRAMES = 8\n",
    "INPUT_NUM_SHOTS = 16\n",
    "INPUT_NUM_SAMPLES = 513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 513])\n",
      "tensor([[[ 1.0697,  0.2883,  1.6725,  ...,  1.2902,  0.5838,  0.3887],\n",
      "         [ 0.2202,  1.3064,  0.5272,  ..., -0.4052, -0.1754, -0.4436],\n",
      "         [ 0.4609,  0.9690,  1.0313,  ..., -0.0095, -1.4749,  0.1886],\n",
      "         ...,\n",
      "         [-0.5537, -1.1102, -0.6680,  ...,  0.9722, -0.4078, -0.1424],\n",
      "         [-1.7581,  0.9624,  0.8037,  ...,  0.6051,  0.3907, -1.3744],\n",
      "         [ 1.7978, -0.1625, -0.0728,  ..., -1.4555,  0.2482,  1.4557]],\n",
      "\n",
      "        [[-0.4664,  0.3054, -0.3222,  ..., -0.2710, -1.4407,  0.3710],\n",
      "         [-0.1934,  0.5433,  1.0452,  ...,  0.0475, -0.3893,  1.4970],\n",
      "         [-0.5600, -0.8551,  1.9716,  ..., -1.3721,  0.4243, -0.4418],\n",
      "         ...,\n",
      "         [ 1.0912, -0.5216,  0.0585,  ...,  0.1243, -0.9368, -0.4834],\n",
      "         [-0.1158, -0.6875,  0.5857,  ..., -2.4228, -2.1988, -1.0237],\n",
      "         [-1.0588,  1.7509, -0.6719,  ..., -0.2302,  0.4094,  0.8756]],\n",
      "\n",
      "        [[-0.2417,  0.6076,  0.3813,  ..., -1.4281, -0.4077,  0.7200],\n",
      "         [-0.2546,  0.4152, -0.1244,  ..., -0.7829, -1.1176, -0.4450],\n",
      "         [ 0.4658,  0.4019,  0.5013,  ...,  0.6537,  1.2446,  0.4274],\n",
      "         ...,\n",
      "         [ 0.5897,  0.4571, -2.0939,  ...,  0.4622,  0.1416, -0.2748],\n",
      "         [-1.1187, -1.5290,  1.3790,  ..., -0.6149,  1.2127, -0.1136],\n",
      "         [-1.1693, -0.5946,  1.0065,  ..., -0.1384,  0.4802,  0.6945]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.1460, -0.7046,  1.8788,  ..., -0.2831,  0.9422,  1.1825],\n",
      "         [ 0.3218,  1.3860, -0.3174,  ...,  0.1522,  1.6908, -0.6948],\n",
      "         [ 1.6532,  0.4484,  1.4446,  ...,  0.4825,  1.1488, -1.4340],\n",
      "         ...,\n",
      "         [ 1.0556, -2.1620,  0.2754,  ..., -0.0558,  0.4168, -1.4233],\n",
      "         [ 0.4132,  0.4474,  1.4985,  ...,  0.0587,  0.9852,  0.1773],\n",
      "         [-0.1063,  0.2811, -1.0549,  ...,  0.2562,  2.4116,  0.0207]],\n",
      "\n",
      "        [[ 0.2150,  0.8016,  2.6222,  ..., -0.3615, -0.6088,  1.4174],\n",
      "         [ 1.8593, -1.3501, -1.3953,  ...,  0.0517, -0.2432, -0.8410],\n",
      "         [-0.0660,  1.6108,  1.7982,  ...,  0.0934,  0.3296,  1.5538],\n",
      "         ...,\n",
      "         [ 0.9076, -0.6939,  0.4827,  ..., -0.1748,  0.4037,  1.2467],\n",
      "         [-0.2107, -0.2503, -0.4042,  ..., -1.2473,  1.9521, -2.8760],\n",
      "         [ 1.2621,  0.7873, -0.6979,  ..., -0.5499, -0.9476, -1.5071]],\n",
      "\n",
      "        [[ 0.9912, -0.1709,  0.4391,  ...,  0.8603,  1.5339, -0.5283],\n",
      "         [ 0.0627, -0.8295,  0.5233,  ...,  1.2206, -0.0433,  1.5629],\n",
      "         [-0.3176,  0.7896,  0.6823,  ..., -1.3004,  1.0169,  1.0445],\n",
      "         ...,\n",
      "         [ 0.0736,  0.4254,  0.0383,  ...,  0.3488, -0.1056, -1.2662],\n",
      "         [ 0.7803,  1.7823, -0.7927,  ...,  0.9709,  0.9321,  0.2679],\n",
      "         [-0.6840,  0.6115, -0.6653,  ..., -1.6275,  0.1234, -0.4302]]])\n"
     ]
    }
   ],
   "source": [
    "tensor = th.randn(INPUT_NUM_FRAMES, INPUT_NUM_SHOTS, INPUT_NUM_SAMPLES)\n",
    "print(tensor.shape)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 513])\n",
      "tensor([[[-0.2749, -1.5319,  0.6602,  ...,  0.7589, -0.1699,  0.5049],\n",
      "         [-1.1090, -0.7957,  0.3033,  ..., -0.8509,  0.1895,  0.9395],\n",
      "         [-0.6180, -1.4549,  0.6048,  ...,  0.0263,  1.4170,  0.9824],\n",
      "         ...,\n",
      "         [ 0.0335, -1.2267, -0.8207,  ...,  1.1133,  0.2820,  1.8548],\n",
      "         [-1.2482, -0.3012, -0.2607,  ..., -1.1113,  1.0808,  1.4458],\n",
      "         [-0.4446, -0.6041, -0.7162,  ..., -0.8294, -1.3239,  1.3870]],\n",
      "\n",
      "        [[ 1.4016, -2.6635, -0.5105,  ..., -0.3638, -0.3036,  0.8026],\n",
      "         [-0.7903, -0.9168, -0.1070,  ..., -0.3223,  0.3682,  2.0032],\n",
      "         [-1.4407, -1.0797,  0.1984,  ..., -0.1770,  0.3879,  0.5388],\n",
      "         ...,\n",
      "         [-0.9986, -0.1596, -0.1205,  ...,  0.2285, -0.3145,  1.1340],\n",
      "         [-0.7085, -0.5514, -0.9320,  ..., -0.6109,  0.6219,  0.7748],\n",
      "         [-0.6114, -1.0385, -1.1797,  ..., -0.5507,  0.2498,  0.5992]],\n",
      "\n",
      "        [[-0.2412, -1.3015, -0.0860,  ...,  0.1960, -0.0650,  1.3396],\n",
      "         [ 0.1255, -1.3937, -0.6342,  ..., -1.0076,  0.3021,  1.4983],\n",
      "         [-1.1013, -1.1798, -0.0968,  ...,  0.1738,  1.5365,  0.5723],\n",
      "         ...,\n",
      "         [-0.6502, -1.2595,  1.1398,  ...,  0.5911, -0.1592,  1.0640],\n",
      "         [-1.3246, -1.4561, -0.2202,  ..., -0.7802,  0.9593,  1.6338],\n",
      "         [-0.2517, -0.7532,  0.1182,  ..., -0.2906, -0.8010,  1.3370]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0250, -1.8419, -0.4475,  ...,  0.1690,  0.2949,  1.0698],\n",
      "         [-0.4757,  0.5798, -0.4397,  ..., -0.6733,  0.6827,  1.4828],\n",
      "         [-0.2771, -1.2686, -1.2313,  ...,  0.0808,  0.4254,  1.3352],\n",
      "         ...,\n",
      "         [ 0.5186, -0.8310, -0.0314,  ...,  0.6560,  0.0460,  0.8667],\n",
      "         [-0.8417, -1.0480, -0.5029,  ..., -0.9393,  1.3868,  0.6846],\n",
      "         [-0.8706, -1.2685, -0.8870,  ..., -0.1314, -1.0621,  0.9795]],\n",
      "\n",
      "        [[ 0.4171, -1.4517, -0.5249,  ..., -0.3038,  0.3558,  1.5710],\n",
      "         [-0.5106, -0.6676,  0.2193,  ..., -0.5146, -0.1294,  1.1588],\n",
      "         [-1.4395, -1.6058, -0.6110,  ..., -0.1674,  0.3544,  1.7245],\n",
      "         ...,\n",
      "         [ 0.1890, -1.0322, -0.4818,  ...,  0.7049, -0.5871,  1.1352],\n",
      "         [-0.3897, -1.0137,  0.4520,  ..., -0.8608,  0.7265,  1.7041],\n",
      "         [-0.8353, -0.2305, -1.6389,  ..., -0.6218, -0.2915,  1.3907]],\n",
      "\n",
      "        [[-0.8083, -1.7963,  0.2640,  ...,  0.6848, -0.2836,  1.1979],\n",
      "         [ 0.3048, -2.2235, -0.8466,  ..., -0.2529,  0.1076,  1.8964],\n",
      "         [-0.8807, -1.3656,  0.0823,  ...,  0.2690,  0.7392,  1.0202],\n",
      "         ...,\n",
      "         [-0.7077, -1.2204,  0.4434,  ...,  0.5791,  0.5400,  1.4209],\n",
      "         [-1.2547, -1.0088,  0.3850,  ..., -0.5889,  0.8755,  0.5804],\n",
      "         [-0.2860, -0.9591, -2.1455,  ..., -0.6908, -0.2733,  0.7370]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "src = th.rand(INPUT_NUM_FRAMES, INPUT_NUM_SHOTS, INPUT_NUM_SAMPLES)\n",
    "target = th.rand(INPUT_NUM_FRAMES, INPUT_NUM_SHOTS, INPUT_NUM_SAMPLES)\n",
    "tranformer = th.nn.Transformer(d_model=INPUT_NUM_SAMPLES, nhead=3, num_encoder_layers=6)\n",
    "output = tranformer(src, target)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 16, 513])\n",
      "tensor([[[[ 1.7223e-01,  3.3111e-01,  1.6417e-01,  ..., -1.1748e-01,\n",
      "            3.6619e-01,  4.2179e-03],\n",
      "          [ 3.1119e-01,  3.6069e-01, -1.6435e-02,  ...,  1.5901e-01,\n",
      "            3.9913e-01,  2.8942e-01],\n",
      "          [ 4.8629e-01,  1.6831e-01,  3.3891e-01,  ...,  2.6586e-01,\n",
      "            7.4845e-02,  3.1926e-01],\n",
      "          ...,\n",
      "          [ 3.1736e-01,  3.5617e-01,  1.8920e-01,  ...,  3.7205e-01,\n",
      "            3.6132e-01,  3.9979e-01],\n",
      "          [ 4.6121e-01, -2.5154e-02,  4.4649e-01,  ...,  4.9748e-02,\n",
      "            3.7322e-01,  1.4345e-01],\n",
      "          [ 3.4014e-01, -8.4815e-02,  2.3315e-01,  ..., -6.1690e-02,\n",
      "            2.0942e-01,  1.8454e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 8.7615e-02,  1.7760e-01,  3.1913e-03,  ...,  2.2826e-01,\n",
      "            6.3584e-02,  2.0334e-01],\n",
      "          [ 3.8730e-01,  2.3870e-01,  3.2886e-01,  ...,  1.0628e-01,\n",
      "            5.2251e-01,  1.9734e-01],\n",
      "          [ 1.2287e-01,  1.3198e-01,  5.2217e-01,  ...,  2.8285e-01,\n",
      "            2.5804e-01,  3.0729e-01],\n",
      "          ...,\n",
      "          [ 5.6260e-01,  3.5778e-02,  3.7406e-01,  ...,  1.1299e-01,\n",
      "            2.1455e-01,  3.3630e-01],\n",
      "          [ 4.8930e-01,  3.8751e-02,  3.7452e-01,  ...,  2.4928e-01,\n",
      "            1.7554e-01,  3.8857e-01],\n",
      "          [ 2.2507e-01,  2.3982e-01, -6.5796e-02,  ...,  9.1673e-02,\n",
      "           -1.5718e-01,  4.3390e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 9.2430e-02,  1.2572e-01,  1.3589e-01,  ...,  3.5525e-01,\n",
      "            5.5605e-02,  1.5717e-01],\n",
      "          [ 2.1743e-01,  2.0475e-01,  8.4050e-02,  ...,  5.5901e-01,\n",
      "           -4.7207e-02,  4.4327e-01],\n",
      "          [ 3.1431e-01, -4.6502e-02,  3.6902e-01,  ...,  2.6178e-01,\n",
      "            1.3368e-01,  3.6691e-01],\n",
      "          ...,\n",
      "          [ 1.1001e-01,  2.3170e-01,  1.3995e-01,  ...,  2.3004e-01,\n",
      "            1.6590e-03,  3.3425e-01],\n",
      "          [ 1.5367e-01,  3.5738e-01,  8.2790e-02,  ...,  9.6249e-02,\n",
      "            3.3057e-01,  2.1624e-01],\n",
      "          [ 2.0431e-01, -3.6890e-02, -4.2130e-02,  ..., -8.1592e-02,\n",
      "            3.0592e-01, -1.0288e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 6.2512e-02,  8.3284e-02,  7.4216e-02,  ...,  1.3993e-01,\n",
      "            1.3775e-01,  2.0878e-01],\n",
      "          [ 2.2655e-01,  2.0231e-01,  3.0906e-01,  ...,  1.7196e-01,\n",
      "            2.4740e-01,  3.6011e-01],\n",
      "          [ 4.6550e-01,  1.7298e-01,  2.5372e-01,  ...,  3.5647e-01,\n",
      "            1.7776e-01,  5.9528e-01],\n",
      "          ...,\n",
      "          [ 8.0879e-02,  3.4549e-01,  1.2326e-01,  ...,  4.8286e-01,\n",
      "            4.0188e-01,  2.0295e-01],\n",
      "          [ 1.0080e-01,  2.7553e-01,  2.6985e-01,  ...,  1.9334e-01,\n",
      "            4.0358e-01,  5.6102e-02],\n",
      "          [ 1.0975e-01,  1.6928e-01, -4.7758e-02,  ...,  1.3364e-01,\n",
      "            5.4220e-04,  2.4809e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.6957e-01,  9.2489e-02,  1.7173e-02,  ..., -6.9803e-02,\n",
      "            1.5453e-01,  1.6001e-01],\n",
      "          [ 4.3549e-01,  2.2873e-01,  2.0250e-01,  ...,  7.3819e-02,\n",
      "            1.7884e-01,  5.2575e-01],\n",
      "          [ 3.0694e-01,  5.1359e-01,  1.3975e-01,  ..., -1.2595e-01,\n",
      "            1.7021e-01,  4.2723e-01],\n",
      "          ...,\n",
      "          [ 3.3175e-01,  2.6465e-01,  2.6333e-01,  ...,  3.0018e-01,\n",
      "            2.8651e-01,  2.6131e-01],\n",
      "          [ 8.2845e-02,  1.4874e-01,  3.3342e-01,  ...,  2.1366e-01,\n",
      "            6.5070e-02,  3.6814e-01],\n",
      "          [ 1.2082e-01,  1.9448e-01,  2.2014e-01,  ...,  8.4688e-02,\n",
      "           -2.6726e-02,  4.2164e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3095e-01,  3.6804e-02,  3.0498e-01,  ...,  1.7302e-01,\n",
      "           -4.7212e-02,  1.1819e-01],\n",
      "          [ 1.0838e-01,  2.1324e-01,  5.6412e-01,  ...,  2.8183e-01,\n",
      "            1.1499e-01,  3.1682e-01],\n",
      "          [-2.1120e-02,  4.6654e-01,  2.8386e-01,  ...,  1.8073e-01,\n",
      "            2.2007e-01,  2.4875e-01],\n",
      "          ...,\n",
      "          [ 1.9528e-01,  2.2215e-01,  2.5865e-01,  ..., -2.9725e-02,\n",
      "            2.2958e-01,  2.2411e-01],\n",
      "          [ 3.2689e-01,  1.6568e-01,  2.6637e-01,  ..., -5.3621e-02,\n",
      "            4.4877e-01,  1.9333e-02],\n",
      "          [ 1.7654e-01,  6.1033e-02,  1.1759e-01,  ..., -2.2736e-01,\n",
      "            1.7801e-01,  7.1871e-02]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "conv_layer = th.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# Reshape the input tensor to match the expected input shape of Conv2d\n",
    "# Conv2d expects input of shape (batch_size, channels, height, width)\n",
    "# Here, we treat INPUT_NUM_FRAMES as batch_size\n",
    "reshaped_tensor = src.view(INPUT_NUM_FRAMES, 1, INPUT_NUM_SHOTS, INPUT_NUM_SAMPLES)\n",
    "\n",
    "# Apply the convolutional layer\n",
    "conv_output = conv_layer(reshaped_tensor)\n",
    "print(conv_output.shape)\n",
    "print(conv_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 513])\n"
     ]
    }
   ],
   "source": [
    "#Full Flow\n",
    "# Reshape the input tensor to match the expected input shape of Conv2d\n",
    "# Conv2d expects input of shape (batch_size, channels, height, width)\n",
    "# Here, we treat INPUT_NUM_FRAMES as batch_size\n",
    "reshaped_tensor = src.view(INPUT_NUM_FRAMES, 1, INPUT_NUM_SHOTS, INPUT_NUM_SAMPLES)\n",
    "\n",
    "# Apply the convolutional layer\n",
    "conv_output = conv_layer(reshaped_tensor)\n",
    "#print(conv_output.shape)\n",
    "\n",
    "# Reshape the output tensor to match the expected input shape of Transformer\n",
    "# Transformer expects input of shape (sequence_length, batch_size, d_model)\n",
    "# Here, we treat INPUT_NUM_SHOTS as sequence_length and INPUT_NUM_SAMPLES as d_model\n",
    "reshaped_tensor = conv_output.view(INPUT_NUM_FRAMES, INPUT_NUM_SHOTS, INPUT_NUM_SAMPLES)    \n",
    "print(reshaped_tensor.shape)\n",
    "\n",
    "# Apply the transformer\n",
    "output = tranformer(reshaped_tensor, reshaped_tensor)\n",
    "#print(output.shape)\n",
    "\n",
    "#print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
